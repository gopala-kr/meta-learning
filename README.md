



> Meta-learning research

--------------

|| [maml_rl](https://github.com/cbfinn/maml_rl) | [Meta-RL](https://github.com/awjuliani/Meta-RL) | [learning-to-learn](https://github.com/deepmind/learning-to-learn) | [supervised-reptile](https://github.com/openai/supervised-reptile) | [pytorch-maml-rl](https://github.com/tristandeleu/pytorch-maml-rl) | [metacar](https://github.com/thibo73800/metacar) | [pytorch-meta-optimizer](https://github.com/ikostrikov/pytorch-meta-optimizer) | [TCML-tensorflow](https://github.com/devsisters/TCML-tensorflow) | [awesome-architecture-search](https://github.com/markdtw/awesome-architecture-search) | [awesome-meta-learning](https://github.com/dragen1860/awesome-meta-learning) | [Meta-Learning-Papers](https://github.com/floodsung/Meta-Learning-Papers) | [awesome-NAS](https://github.com/D-X-Y/awesome-NAS) | [google-research/nasbench](https://github.com/google-research/nasbench) | [[AlphaX-NASBench101](https://github.com/linnanwang/AlphaX-NASBench101)] | [paperswithcode: meta-learning](https://paperswithcode.com/task/meta-learning) | [paperswithcode: architecture-search](https://paperswithcode.com/task/architecture-search) ||



-------------------


Review papers


- [Multi-Objective Reinforced Evolution in Mobile Neural Architecture Search](https://arxiv.org/abs/1901.01074v2) (2019)
- [EAT-NAS: Elastic Architecture Transfer for Accelerating Large-scale Neural Architecture Search](https://arxiv.org/abs/1901.05884v1) (2019)
- [Fast, Accurate and Lightweight Super-Resolution with Neural Architecture Search](https://arxiv.org/abs/1901.07261v2) (2019)
- [Hierarchical Critics Assignment for Multi-agent Reinforcement Learning](https://arxiv.org/abs/1902.03079v1) (2019)
- [BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning](https://arxiv.org/abs/1902.02671v1) (2019)
- [CESMA: Centralized Expert Supervises Multi-Agents](https://arxiv.org/abs/1902.02311v2) (2019)
- [Probabilistic Neural Architecture Search](https://arxiv.org/abs/1902.05116v1) (2019)
- [Random Search and Reproducibility for Neural Architecture Search](https://arxiv.org/abs/1902.07638v1) (2019)
- [Evaluating the Search Phase of Neural Architecture Search](https://arxiv.org/abs/1902.08142v1) (2019)
- [Evolutionary Neural AutoML for Deep Learning](https://arxiv.org/abs/1902.06827v1) (2019)
- [AutoQB: AutoML for Network Quantization and Binarization on Mobile Devices](https://arxiv.org/abs/1902.05690v1) (2019)
- [Online Meta-Learning](https://arxiv.org/pdf/1902.08438v1.pdf) (2019)
- [NAS-Bench-101: Towards Reproducible Neural Architecture Search](https://arxiv.org/abs/1902.09635v1) (2019) 
- [Concurrent Meta Reinforcement Learning](https://arxiv.org/abs/1903.02710v1) (2019)
- [NoRML: No-Reward Meta Learning](https://arxiv.org/abs/1903.01063v1) (2019)
- [Provable Guarantees for Gradient-Based Meta-Learning](https://arxiv.org/abs/1902.10644v1) (2019)
- [MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning](https://arxiv.org/abs/1903.10258v1) (2019)
- [DetNAS: Neural Architecture Search on Object Detection](https://arxiv.org/abs/1903.10979v1) (2019)
- [AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search](https://arxiv.org/abs/1903.11059v1) (2019)
- [ASAP: Architecture Search, Anneal and Prune](https://arxiv.org/abs/1904.04123v1) (2019)
- [Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation](https://arxiv.org/abs/1901.02985v2) (2019)
- [Meta-Learning surrogate models for sequential decision making](https://arxiv.org/abs/1903.11907v1) (2019)
- [Hierarchical Meta Learning](https://arxiv.org/abs/1904.09081v1) (2019)
- [AM-LFS: AutoML for Loss Function Search](https://arxiv.org/abs/1905.07375v1) (2019)
- [Meta-Learning with Latent Embedding Optimization](https://arxiv.org/abs/1807.05960)  (2019) [[Code](https://github.com/deepmind/leo)]
------------------------

- [Deep Reinforcement Learning for Multi-Agent Systems: A Review of Challenges, Solutions and Applications](https://arxiv.org/abs/1812.11794v1)
- [Meta Reinforcement Learning with Distribution of Exploration Parameters Learned by Evolution Strategies](https://arxiv.org/abs/1812.11314v1) (2018)
- [Deep Reinforcement Learning for Multi-Agent Systems: A Review of Challenges, Solutions and Applications](https://arxiv.org/abs/1812.11794v1) (2018)
- [Rethink and Redesign Meta learning](https://arxiv.org/abs/1812.04955v2) (2018)
- [Taking Human out of Learning Applications: A
Survey on Automated Machine Learning](https://arxiv.org/pdf/1810.13306v1.pdf) (2018)
- [Neural Architecture Search: A Survey](https://arxiv.org/pdf/1808.05377v1.pdf) (2018)
- [SNAS: Stochastic Neural Architecture Search](https://arxiv.org/abs/1812.09926v1) (2018)
- [A Review of Meta-Reinforcement Learning for Deep Neural Networks Architecture Search](https://arxiv.org/abs/1812.07995v1) (2018)
- [FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search](https://arxiv.org/abs/1812.03443v2) (2018)
- [Evolutionary Neural Architecture Search for Image Restoration](https://arxiv.org/abs/1812.05866v1) (2018)
- [MONAS: Multi-Objective Neural Architecture Search using Reinforcement Learning](https://arxiv.org/abs/1806.10332v2) (2018)
- [DSO-NAS](https://arxiv.org/pdf/1811.01567v1.pdf) (2018)
- [Auto-Keras: Efficient Neural Architecture Search with Network Morphism](https://arxiv.org/pdf/1806.10282v2.pdf) (2018)
- [Visual Analytics for Automated Model Discovery](https://arxiv.org/pdf/1809.10782v2.pdf) (2018)
- [AlphaX: eXploring Neural Architectures with Deep
Neural Networks and Monte Carlo Tree Search](https://arxiv.org/pdf/1805.07440v1.pdf) (2018)
- [DARTS: Differentiable Architecture Search](https://arxiv.org/pdf/1806.09055v1.pdf) (2018)
- [Towards Automated Deep Learning: Efficient Joint Neural Architecture and Hyperparameter Search](https://arxiv.org/abs/1807.06906v1) (2018)
- [Fast Neural Architecture Search of Compact Semantic Segmentation Models
via Auxiliary Cellsâˆ—](https://arxiv.org/pdf/1810.10804v1.pdf) (2018)
- [Meta Learning Deep Visual Words for Fast Video Object Segmentation](https://arxiv.org/abs/1812.01397v1) (2018)
- [Representation based and Attention augmented Meta learning](https://arxiv.org/abs/1811.07545v3) (2018)
-  Meta-Learning for Semi-Supervised Few-Shot Classification.[[paper](https://openreview.net/pdf?id=HJcSzz-CZ)] ICLR 2018. [[code](https://github.com/renmengye/few-shot-ssl-public)]
-  Machine Theory of Mind. [[arxiv](https://arxiv.org/pdf/1802.07740.pdf)] (2018).
-  Meta-Gradient Reinforcement Learning. [[arxiv](https://arxiv.org/pdf/1805.09801.pdf)] (2018).
-  Learning a Prior over Intent via Meta-Inverse Reinforcement Learning. [[arxiv](https://arxiv.org/pdf/1805.12573.pdf)] (2018).
-  Probabilistic Model-Agnostic Meta-Learning. [[arxiv](https://arxiv.org/pdf/1806.02817.pdf)] (2018).
-  Unsupervised Meta-Learning for Reinforcement Learning. [[arxiv](https://arxiv.org/pdf/1806.04640.pdf)](2018).
-  Meta Learner with Linear Nulling. [[arxiv](https://arxiv.org/pdf/1806.01010.pdf)] (2018).
-  Bayesian Model-Agnostic Meta-Learning. [[arxiv](https://arxiv.org/pdf/1806.03836.pdf)] (2018).
-  Meta-Reinforcement Learning of Structured Exploration Strategies. [[arxiv](https://arxiv.org/pdf/1802.07245.pdf)] (2018).
-  Learning to Adapt: Meta-Learning for Model-Based Control. [[arxiv](https://arxiv.org/pdf/1803.11347.pdf)] (2018).
-  Evolved policy gradients. [[openai](https://blog.openai.com/evolved-policy-gradients/)] (2018).
-  Learning to Explore with Meta-Policy Gradient. [[arxiv](https://arxiv.org/pdf/1803.05044.pdf)] (2018).
-  Some considerations on learning to explore via meta-reinforcement learning. [[arxiv](https://arxiv.org/pdf/1803.01118.pdf)] (2018).
-  Meta-learning with differentiable closed-form solvers. [[arxiv](https://arxiv.org/pdf/1805.08136.pdf)] (2018).
-  Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace. [[arxiv](https://arxiv.org/pdf/1801.05558.pdf)] ICML 2018.


-------------

-  Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions. [[arxiv](https://arxiv.org/pdf/1710.10304.pdf)] (2017).
-  Learning to learn by gradient descent by gradient descent. [[arxiv](https://arxiv.org/abs/1606.04474)] , 2016, [[code](https://github.com/ikostrikov/pytorch-meta-optimizer)]
-  Using fast weights to attend to the recent past. [[arxiv](https://arxiv.org/pdf/1610.06258.pdf)]  2016
-  Hypernetworks. In ICLR 2017, [[arxiv](https://arxiv.org/pdf/1609.09106.pdf)] .
-  Siamese neural networks for one-shot image recognition. [[arxiv](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)]
-  One-shot learning by inverting a compositional causal process.[[arxiv](https://cims.nyu.edu/~brenden/LakeEtAlNips2013.pdf)]  2013.
-  Meta-learning with memory-augmented neural networks.[[arxiv](http://proceedings.mlr.press/v48/santoro16.pdf)]  2016.
-  Matching networks for one shot learning.[[arxiv](https://arxiv.org/pdf/1606.04080.pdf)]  2016.
-  Learning to remember rare events.[[arxiv](https://arxiv.org/pdf/1703.03129.pdf)] In ICLR 2017.
-  Learning to navigate in complex environments.[[arxiv](https://arxiv.org/pdf/1611.03673.pdf)]  DeepMind, 2016.
-  Neural architecture search with reinforcement learning. [[arxiv](https://arxiv.org/pdf/1611.01578.pdf)] ICLR 2017.
-  Rl2: Fast reinforcement learning via slow reinforcement learning.  UC Berkeley and OpenAI,[[arxiv](https://arxiv.org/pdf/1611.02779.pdf)] 2016.
-  Learning to optimize. (ICLR),[[arxiv](https://arxiv.org/pdf/1606.01885.pdf)]  2017.
-  Towards a neural statistician.[[arxiv](https://arxiv.org/pdf/1606.02185.pdf)]  (ICLR), 2017.
-  Actor-mimic: Deep multitask and transfer reinforcement learning. [[arxiv](https://arxiv.org/pdf/1511.06342.pdf)] (ICLR), 2016.
-  Optimization as a model for few-shot learning. [[arxiv](https://openreview.net/pdf?id=rJY0-Kcll)] (ICLR), 2017.
-  Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.[[arxiv](https://arxiv.org/pdf/1703.03400.pdf)] , [[code](https://github.com/cbfinn/maml)], [[pytorch-maml-rl](https://github.com/tristandeleu/pytorch-maml-rl)], [[code](https://github.com/cbfinn/maml_rl)]
-  Learning to Learn for Global Optimization of Black Box Functions. [[arxiv](https://arxiv.org/pdf/1611.03824v1.pdf)]
-  Meta Networks.[[arxiv](https://arxiv.org/pdf/1703.00837.pdf)]  2017.
-  One-Shot Imitation Learning.[[arxiv](https://arxiv.org/pdf/1703.07326.pdf)]  2017.
-  Active One-shot Learning.[[arxiv](https://arxiv.org/pdf/1702.06559.pdf)]  2017.
-  Learned Optimizers that Scale and Generalize.[[arxiv](https://arxiv.org/pdf/1703.04813.pdf)]  2017.
-  Low-shot visual object recognition (2016).[[arxiv](https://arxiv.org/pdf/1606.02819.pdf)]
-  Learning to reinforcement learn.[[arxiv](https://arxiv.org/pdf/1611.05763.pdf)]  2016.  [[code](https://github.com/awjuliani/Meta-RL)]
-  Learning to Learn: Meta-Critic Networks for Sample Efficient Learning. [[arxiv](https://arxiv.org/pdf/1706.09529.pdf)] 2017.
-  Meta-SGD: Learning to Learn Quickly for Few Shot Learning. [[arxiv](https://arxiv.org/pdf/1707.09835.pdf)] 2017.
-  Meta-Learning with Temporal Convolutions.[[arxiv](https://arxiv.org/pdf/1707.03141.pdf)]  2017. [[code](https://github.com/devsisters/TCML-tensorflow)]
-  Meta Learning Shared Hierarchies.[[arxiv](https://arxiv.org/pdf/1710.09767.pdf)]  2017.
-  One-shot visual imitation learning via meta-learning. [[arxiv](https://arxiv.org/pdf/1709.04905.pdf)] 2017.  [[code](https://github.com/tianheyu927/mil)]
-  Learning to Compare: Relation Network for Few Shot Learning. [[arxiv](https://arxiv.org/pdf/1711.06025.pdf)] 2017.
-  Human-level concept learning through probabilistic program induction.[[arxiv](http://web.mit.edu/cocosci/Papers/Science-2015-Lake-1332-8.pdf)]  2015.
-  Neural task programming: Learning to generalize across hierarchical tasks. [[arxiv](https://arxiv.org/pdf/1710.01813.pdf)] 2017.
-  Learning feed-forward one-shot learners. [[arxiv](https://arxiv.org/pdf/1606.05233.pdf)]
-  Learning to learn: Model regression networks for easy small sample learning. [[arxiv](https://www.robots.ox.ac.uk/~vgg/rg/papers/eccv2016_learntolearn.pdf)] 2016.

-------------

- Meta-learning in reinforcement learning. [[paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.595.9882&rep=rep1&type=pdf)] 2003.
- Learning to learn using gradient descent. [[paper](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=39AFD770239F0DDE818EEF5BAEBB0814?doi=10.1.1.5.323&rep=rep1&type=pdf)] 2001.
- A meta-learning method based on temporal difference error. [[paper](http://www.ist.aichi-pu.ac.jp/~koba/files/ICONIP09.pdf)]  2009.
- Learning to learn: Introduction and overview. [[paper](https://link.springer.com/chapter/10.1007/978-1-4615-5529-2_1)]  1998.
-  Meta-learning with backpropagation. [[paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.363.3955&rep=rep1&type=pdf)]  2001.
-  A perspective view and survey of meta-learning. [[paper](https://www.researchgate.net/publication/2375370_A_Perspective_View_And_Survey_Of_Meta-Learning)]  2002.
-  Zero-data learning of new tasks. [[paper](https://www.aaai.org/Papers/AAAI/2008/AAAI08-103.pdf)]  2008.
- One shot learning of simple visual concepts. [[paper](https://cims.nyu.edu/~brenden/LakeEtAl2011CogSci.pdf)]  2011.
- One-shot learning of object categories. [[paper](https://authors.library.caltech.edu/5407/1/LIFieeetpam06.pdf)] 2006.
- A neural network that embeds its own meta-levels. [[paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.4997&rep=rep1&type=pdf)]  1993.
-  Lifelong learning algorithms. [[paper](https://link.springer.com/chapter/10.1007/978-1-4615-5529-2_8)]  1998.
-  Learning a synaptic learning rule.  [[paper](http://bengio.abracadoudou.com/publications/pdf/bengio_1991_ijcnn.pdf)]  1990.
- On the search for new learning rules for ANNs.  [[paper](http://bengio.abracadoudou.com/cv/publications/pdf/bengio_1995_npl.pdf)]  1995.
-  Learning many related tasks at the same time with backpropagation. [[paper](https://pdfs.semanticscholar.org/210d/a45e57f86a50c04bdd7b37d498c8ecc288da.pdf)]  1995.
- Introduction to the special issue on meta-learning. [[paper](http://www2.cs.uh.edu/~vilalta/papers/ml04.pdf)]  2004.
-  Meta-learning in computational intelligence. [[paper](https://www.researchgate.net/publication/321613968_Meta-Learning_in_Computational_Intelligence)]  2011.
- Fixed-weight networks can learn. [[paper](https://www.researchgate.net/publication/224755261_Fixed-weight_networks_can_learn)]   1990.
- Evolutionary principles in self-referential learning; On learning how to learn: The meta-meta-...
hook. [[paper](http://people.idsia.ch/~juergen/diploma.html)]  1987.
-  Learning to control fast-weight memories: An alternative to dynamic recurrent networks.Neural Computation, [[paper](http://people.idsia.ch/~juergen/fastweights/ncfastweightsrev.html)]  1992.
-  Simple principles of metalearning. [[paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.2558&rep=rep1&type=pdf)]  1996.
-  Learning to learn.  [[paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.2558&rep=rep1&type=pdf)]  1998.

-------------
**Maintainer**

Gopala KR / @gopala-kr
